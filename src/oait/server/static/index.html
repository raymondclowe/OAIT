<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OAIT - Observational AI Tutor</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
        }
        
        h1 {
            color: #667eea;
            margin-bottom: 10px;
        }
        
        .subtitle {
            color: #666;
            margin-bottom: 30px;
        }
        
        .controls {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
            flex-wrap: wrap;
        }
        
        button {
            padding: 12px 24px;
            border: none;
            border-radius: 8px;
            font-size: 16px;
            cursor: pointer;
            transition: all 0.3s;
            font-weight: 600;
        }
        
        button:hover:not(:disabled) {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }
        
        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        .btn-primary {
            background: #667eea;
            color: white;
        }
        
        .btn-danger {
            background: #f56565;
            color: white;
        }
        
        .btn-success {
            background: #48bb78;
            color: white;
        }
        
        .status {
            padding: 12px;
            border-radius: 8px;
            margin-bottom: 20px;
            font-weight: 500;
        }
        
        .status.connected {
            background: #c6f6d5;
            color: #22543d;
        }
        
        .status.disconnected {
            background: #fed7d7;
            color: #742a2a;
        }
        
        .workspace {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-bottom: 20px;
        }
        
        .panel {
            background: #f7fafc;
            border-radius: 12px;
            padding: 20px;
        }
        
        .panel h2 {
            color: #2d3748;
            margin-bottom: 15px;
            font-size: 18px;
        }
        
        #whiteboard {
            width: 100%;
            height: 400px;
            border: 2px solid #e2e8f0;
            border-radius: 8px;
            background: white;
        }
        
        .transcript-area, .ai-responses {
            height: 400px;
            overflow-y: auto;
            background: white;
            border: 2px solid #e2e8f0;
            border-radius: 8px;
            padding: 15px;
        }
        
        .transcript-item, .ai-item {
            margin-bottom: 12px;
            padding: 10px;
            border-radius: 6px;
            line-height: 1.5;
        }
        
        .transcript-item {
            background: #ebf8ff;
            border-left: 3px solid #3182ce;
        }
        
        .ai-item {
            background: #f0fff4;
            border-left: 3px solid #38a169;
        }
        
        .timestamp {
            color: #718096;
            font-size: 12px;
            margin-bottom: 4px;
        }
        
        @media (max-width: 768px) {
            .workspace {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üéì OAIT - Observational AI Tutor</h1>
        <p class="subtitle">AI-powered tutoring with continuous observation and intelligent intervention</p>
        
        <div class="controls">
            <input type="text" id="studentId" placeholder="Student ID" style="padding: 12px; border: 2px solid #e2e8f0; border-radius: 8px; flex: 1; min-width: 200px;">
            <button id="startSession" class="btn-primary">Start Session</button>
            <button id="stopSession" class="btn-danger" disabled>Stop Session</button>
            <button id="startAudio" class="btn-success" disabled>Start Audio</button>
            <button id="stopAudio" class="btn-danger" disabled>Stop Audio</button>
        </div>
        
        <div id="status" class="status disconnected">
            ‚ö´ Disconnected
        </div>
        
        <!-- Audio level indicator -->
        <div id="audioLevelContainer" style="display: none; margin-bottom: 15px;">
            <div style="display: flex; align-items: center; gap: 10px;">
                <span>üé§ Audio Level:</span>
                <div style="flex: 1; height: 20px; background: #e2e8f0; border-radius: 10px; overflow: hidden;">
                    <div id="audioLevelBar" style="height: 100%; width: 0%; background: linear-gradient(90deg, #48bb78, #f6e05e, #f56565); transition: width 0.1s;"></div>
                </div>
                <span id="audioStatus" style="min-width: 80px;">Silent</span>
            </div>
        </div>
        
        <div class="workspace">
            <div class="panel">
                <h2>üìù Whiteboard</h2>
                <canvas id="whiteboard"></canvas>
            </div>
            
            <div class="panel">
                <h2>üí¨ Transcript</h2>
                <div id="transcript" class="transcript-area">
                    <p style="color: #a0aec0; text-align: center; margin-top: 100px;">
                        Start speaking to see transcripts...
                    </p>
                </div>
            </div>
        </div>
        
        <div class="panel">
            <h2>ü§ñ AI Responses</h2>
            <div id="aiResponses" class="ai-responses">
                <p style="color: #a0aec0; text-align: center; margin-top: 50px;">
                    AI responses will appear here...
                </p>
            </div>
        </div>
        
        <div class="panel" style="margin-top: 20px;">
            <h2>üîß Debug Log <button id="clearDebug" style="padding: 4px 8px; font-size: 12px; margin-left: 10px;">Clear</button></h2>
            <div id="debugLog" class="debug-area" style="height: 200px; overflow-y: auto; background: #1a202c; border: 2px solid #4a5568; border-radius: 8px; padding: 15px; font-family: monospace; font-size: 12px; color: #68d391;">
                <p style="color: #a0aec0;">Debug messages will appear here...</p>
            </div>
        </div>
    </div>
    
    <script>
        // =====================================================
        // PULL-BASED ARCHITECTURE
        // Client buffers data locally, server requests when needed
        // =====================================================
        
        let ws = null;
        let sessionId = null;
        let audioContext = null;
        let mediaStream = null;
        let isRecording = false;
        let canvas = null;
        let ctx = null;
        
        // =====================================================
        // CLIENT-SIDE BUFFERS (AI pulls from these on demand)
        // =====================================================
        const clientBuffers = {
            // Audio buffer - stores recent speech segments
            audio: {
                segments: [],           // Array of {data: Float32Array, timestamp: number}
                maxDurationMs: 30000,   // Keep 30 seconds of audio
                currentSegment: [],     // Current speech being recorded
                speechStartTime: null,
                lastSpeechTime: null,
            },
            // Whiteboard - latest canvas state
            whiteboard: {
                lastChange: null,       // Timestamp of last drawing
                hasChanges: false,      // Whether whiteboard changed since last request
            },
            // Transcript - recent speech transcripts from server
            transcripts: [],            // Array of {text, timestamp}
        };
        
        const statusDiv = document.getElementById('status');
        const startSessionBtn = document.getElementById('startSession');
        const stopSessionBtn = document.getElementById('stopSession');
        const startAudioBtn = document.getElementById('startAudio');
        const stopAudioBtn = document.getElementById('stopAudio');
        const studentIdInput = document.getElementById('studentId');
        const transcriptDiv = document.getElementById('transcript');
        const aiResponsesDiv = document.getElementById('aiResponses');
        const debugLogDiv = document.getElementById('debugLog');
        const clearDebugBtn = document.getElementById('clearDebug');
        
        // Clear debug log
        clearDebugBtn.addEventListener('click', () => {
            debugLogDiv.innerHTML = '<p style="color: #a0aec0;">Debug cleared...</p>';
        });
        
        // Initialize canvas
        canvas = document.getElementById('whiteboard');
        ctx = canvas.getContext('2d');
        canvas.width = canvas.offsetWidth;
        canvas.height = canvas.offsetHeight;
        
        // Simple drawing functionality with change tracking
        let isDrawing = false;
        let lastX = 0;
        let lastY = 0;
        
        canvas.addEventListener('mousedown', (e) => {
            isDrawing = true;
            [lastX, lastY] = [e.offsetX, e.offsetY];
        });
        
        canvas.addEventListener('mousemove', (e) => {
            if (!isDrawing) return;
            ctx.beginPath();
            ctx.moveTo(lastX, lastY);
            ctx.lineTo(e.offsetX, e.offsetY);
            ctx.strokeStyle = '#2d3748';
            ctx.lineWidth = 2;
            ctx.lineCap = 'round';
            ctx.stroke();
            [lastX, lastY] = [e.offsetX, e.offsetY];
            
            // Mark whiteboard as changed
            clientBuffers.whiteboard.lastChange = Date.now();
            clientBuffers.whiteboard.hasChanges = true;
        });
        
        canvas.addEventListener('mouseup', () => {
            isDrawing = false;
        });
        
        canvas.addEventListener('mouseout', () => {
            isDrawing = false;
        });
        
        // =====================================================
        // RESPOND TO SERVER REQUESTS (AI Tool Calls)
        // =====================================================
        function handleServerRequest(request) {
            const requestId = request.request_id;
            const resource = request.resource;
            
            addDebug(`üì• AI requested: ${resource}`);
            console.log('[REQUEST]', resource, requestId);
            
            if (resource === 'audio') {
                // Send buffered audio segments
                const audioData = getBufferedAudio();
                ws.send(JSON.stringify({
                    type: 'response',
                    request_id: requestId,
                    resource: 'audio',
                    data: audioData.base64,
                    duration_ms: audioData.durationMs,
                    has_speech: audioData.hasSpeech,
                }));
                addDebug(`üì§ Sent ${audioData.durationMs}ms audio (speech: ${audioData.hasSpeech})`);
                
            } else if (resource === 'whiteboard') {
                // Send current whiteboard image
                canvas.toBlob((blob) => {
                    const reader = new FileReader();
                    reader.onload = () => {
                        const base64 = reader.result.split(',')[1];
                        ws.send(JSON.stringify({
                            type: 'response',
                            request_id: requestId,
                            resource: 'whiteboard',
                            data: base64,
                            has_changes: clientBuffers.whiteboard.hasChanges,
                            last_change: clientBuffers.whiteboard.lastChange,
                        }));
                        // Reset change tracking
                        clientBuffers.whiteboard.hasChanges = false;
                        addDebug(`üì§ Sent whiteboard image`);
                    };
                    reader.readAsDataURL(blob);
                }, 'image/png');
                
            } else if (resource === 'status') {
                // Send client status summary
                ws.send(JSON.stringify({
                    type: 'response',
                    request_id: requestId,
                    resource: 'status',
                    data: {
                        audio_recording: isRecording,
                        audio_segments: clientBuffers.audio.segments.length,
                        whiteboard_has_changes: clientBuffers.whiteboard.hasChanges,
                        transcript_count: clientBuffers.transcripts.length,
                    }
                }));
                addDebug(`üì§ Sent status`);
            }
        }
        
        // Get buffered audio as WAV base64
        function getBufferedAudio() {
            // Combine all audio segments
            const allSamples = [];
            let totalLength = 0;
            
            for (const segment of clientBuffers.audio.segments) {
                totalLength += segment.data.length;
                allSamples.push(segment.data);
            }
            
            // Also include current segment if any
            for (const chunk of clientBuffers.audio.currentSegment) {
                totalLength += chunk.length;
                allSamples.push(chunk);
            }
            
            if (totalLength === 0) {
                return { base64: '', durationMs: 0, hasSpeech: false };
            }
            
            // Combine into single array
            const combined = new Float32Array(totalLength);
            let offset = 0;
            for (const samples of allSamples) {
                combined.set(samples, offset);
                offset += samples.length;
            }
            
            const sampleRate = audioContext ? audioContext.sampleRate : 48000;
            const durationMs = (totalLength / sampleRate * 1000).toFixed(0);
            
            // Convert to WAV
            const wavBlob = float32ToWav(combined, sampleRate);
            
            // Convert to base64 synchronously using array buffer
            return new Promise((resolve) => {
                const reader = new FileReader();
                reader.onload = () => {
                    const base64 = reader.result.split(',')[1];
                    // Clear the segments after sending
                    clientBuffers.audio.segments = [];
                    resolve({ base64, durationMs: parseInt(durationMs), hasSpeech: true });
                };
                reader.readAsDataURL(wavBlob);
            });
        }
        
        // =====================================================
        // SESSION MANAGEMENT
        // =====================================================
        startSessionBtn.addEventListener('click', async () => {
            const studentId = studentIdInput.value.trim();
            if (!studentId) {
                alert('Please enter a student ID');
                return;
            }
            
            try {
                // Start session
                const response = await fetch(`/session/start?student_id=${studentId}`, {
                    method: 'POST'
                });
                const data = await response.json();
                sessionId = data.session_id;
                
                // Connect WebSocket
                const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
                ws = new WebSocket(`${protocol}//${window.location.host}/ws/${sessionId}`);
                
                ws.onopen = () => {
                    updateStatus(true);
                    startSessionBtn.disabled = true;
                    stopSessionBtn.disabled = false;
                    startAudioBtn.disabled = false;
                    studentIdInput.disabled = true;
                    addDebug('üîó Connected to server');
                };
                
                ws.onmessage = async (event) => {
                    const message = JSON.parse(event.data);
                    console.log('[WS]', message);
                    
                    if (message.type === 'request') {
                        // AI is requesting data - respond with buffered content
                        const audioResult = await handleServerRequest(message);
                        
                    } else if (message.type === 'transcript') {
                        addTranscript(message.text, message.timestamp);
                        clientBuffers.transcripts.push({text: message.text, timestamp: message.timestamp});
                        // Keep only last 20 transcripts
                        if (clientBuffers.transcripts.length > 20) {
                            clientBuffers.transcripts.shift();
                        }
                        addDebug(`üìù Transcript: ${message.text}`);
                        
                    } else if (message.type === 'ai_response') {
                        addAIResponse(message.text, message.strategy);
                        addDebug(`ü§ñ AI: ${message.text}`);
                        speakText(message.text);
                        
                    } else if (message.type === 'debug') {
                        addDebug(`üîß ${message.message}`);
                        console.log('[DEBUG]', message.message);
                        
                    } else if (message.type === 'pong') {
                        // Heartbeat response
                    } else {
                        console.log('[WS] Unknown message:', message);
                    }
                };
                
                ws.onclose = () => {
                    updateStatus(false);
                    startSessionBtn.disabled = false;
                    stopSessionBtn.disabled = true;
                    startAudioBtn.disabled = true;
                    stopAudioBtn.disabled = true;
                    studentIdInput.disabled = false;
                    addDebug('üîå Disconnected from server');
                };
                
                ws.onerror = (error) => {
                    console.error('WebSocket error:', error);
                    addDebug('‚ùå WebSocket error');
                };
                
            } catch (error) {
                console.error('Error starting session:', error);
                alert('Failed to start session');
            }
        });
        
        stopSessionBtn.addEventListener('click', async () => {
            if (ws) {
                ws.close();
            }
            
            if (sessionId) {
                try {
                    await fetch(`/session/${sessionId}/stop`, {
                        method: 'POST'
                    });
                } catch (error) {
                    console.error('Error stopping session:', error);
                }
            }
            
            sessionId = null;
        });
        
        // =====================================================
        // AUDIO RECORDING (Buffers locally, doesn't send)
        // =====================================================
        startAudioBtn.addEventListener('click', async () => {
            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                audioContext = new AudioContext();
                const source = audioContext.createMediaStreamSource(mediaStream);
                const processor = audioContext.createScriptProcessor(4096, 1, 1);
                
                // Voice Activity Detection (VAD) settings
                const SILENCE_THRESHOLD = 0.01;
                const SILENCE_DURATION_MS = 500;
                const MIN_SPEECH_DURATION_MS = 200;
                const MAX_BUFFER_MS = 30000; // 30 seconds max buffer
                
                // Calculate RMS of audio samples
                function calculateRMS(samples) {
                    let sum = 0;
                    for (let i = 0; i < samples.length; i++) {
                        sum += samples[i] * samples[i];
                    }
                    return Math.sqrt(sum / samples.length);
                }
                
                // Finalize current speech segment
                function finalizeSegment() {
                    if (clientBuffers.audio.currentSegment.length === 0) return;
                    
                    // Combine current segment chunks
                    const totalLength = clientBuffers.audio.currentSegment.reduce((sum, arr) => sum + arr.length, 0);
                    const combined = new Float32Array(totalLength);
                    let offset = 0;
                    for (const chunk of clientBuffers.audio.currentSegment) {
                        combined.set(chunk, offset);
                        offset += chunk.length;
                    }
                    
                    const durationMs = (totalLength / audioContext.sampleRate * 1000);
                    
                    // Only keep if long enough
                    if (durationMs >= MIN_SPEECH_DURATION_MS) {
                        clientBuffers.audio.segments.push({
                            data: combined,
                            timestamp: clientBuffers.audio.speechStartTime,
                        });
                        addDebug(`üíæ Buffered ${durationMs.toFixed(0)}ms speech segment`);
                        
                        // Trim old segments if buffer too large
                        trimAudioBuffer();
                    }
                    
                    // Reset current segment
                    clientBuffers.audio.currentSegment = [];
                    clientBuffers.audio.speechStartTime = null;
                    clientBuffers.audio.lastSpeechTime = null;
                }
                
                // Trim audio buffer to max duration
                function trimAudioBuffer() {
                    let totalDuration = 0;
                    for (const seg of clientBuffers.audio.segments) {
                        totalDuration += seg.data.length / audioContext.sampleRate * 1000;
                    }
                    
                    while (totalDuration > MAX_BUFFER_MS && clientBuffers.audio.segments.length > 1) {
                        const removed = clientBuffers.audio.segments.shift();
                        totalDuration -= removed.data.length / audioContext.sampleRate * 1000;
                    }
                }
                
                source.connect(processor);
                processor.connect(audioContext.destination);
                
                // UI elements for audio level
                const audioLevelContainer = document.getElementById('audioLevelContainer');
                const audioLevelBar = document.getElementById('audioLevelBar');
                const audioStatus = document.getElementById('audioStatus');
                audioLevelContainer.style.display = 'block';
                
                processor.onaudioprocess = (e) => {
                    if (!isRecording) return;
                    
                    const audioData = e.inputBuffer.getChannelData(0);
                    const rms = calculateRMS(audioData);
                    const now = Date.now();
                    
                    // Update audio level indicator
                    const levelPercent = Math.min(100, (rms / 0.3) * 100);
                    audioLevelBar.style.width = levelPercent + '%';
                    
                    if (rms > SILENCE_THRESHOLD) {
                        // Speech detected - buffer it
                        audioStatus.textContent = 'üó£Ô∏è Speaking';
                        audioStatus.style.color = '#38a169';
                        
                        if (!clientBuffers.audio.speechStartTime) {
                            clientBuffers.audio.speechStartTime = now;
                        }
                        clientBuffers.audio.lastSpeechTime = now;
                        clientBuffers.audio.currentSegment.push(new Float32Array(audioData));
                        
                    } else {
                        // Silence
                        audioStatus.textContent = 'üîá Silent';
                        audioStatus.style.color = '#718096';
                        
                        // Check if we should finalize current segment
                        if (clientBuffers.audio.lastSpeechTime && 
                            (now - clientBuffers.audio.lastSpeechTime) >= SILENCE_DURATION_MS) {
                            finalizeSegment();
                        }
                    }
                };
                
                isRecording = true;
                startAudioBtn.disabled = true;
                stopAudioBtn.disabled = false;
                addDebug('üé§ Audio recording started (buffering locally)');
                
            } catch (error) {
                console.error('Error accessing microphone:', error);
                alert('Failed to access microphone');
            }
        });
        
        stopAudioBtn.addEventListener('click', () => {
            isRecording = false;
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
            }
            if (audioContext) {
                audioContext.close();
            }
            document.getElementById('audioLevelContainer').style.display = 'none';
            startAudioBtn.disabled = false;
            stopAudioBtn.disabled = true;
            addDebug('üé§ Audio recording stopped');
        });
        
        // =====================================================
        // UI HELPERS
        // =====================================================
        function updateStatus(connected) {
            if (connected) {
                statusDiv.textContent = 'üü¢ Connected';
                statusDiv.className = 'status connected';
            } else {
                statusDiv.textContent = '‚ö´ Disconnected';
                statusDiv.className = 'status disconnected';
            }
        }
        
        function addTranscript(text, timestamp) {
            const time = new Date(timestamp * 1000).toLocaleTimeString();
            const item = document.createElement('div');
            item.className = 'transcript-item';
            item.innerHTML = `
                <div class="timestamp">${time}</div>
                <div>${text}</div>
            `;
            transcriptDiv.prepend(item);
        }
        
        function addAIResponse(text, strategy) {
            const time = new Date().toLocaleTimeString();
            const item = document.createElement('div');
            item.className = 'ai-item';
            item.innerHTML = `
                <div class="timestamp">${time} ${strategy ? `[${strategy}]` : ''}</div>
                <div>${text}</div>
            `;
            aiResponsesDiv.prepend(item);
        }
        
        function addDebug(message) {
            const time = new Date().toLocaleTimeString();
            const item = document.createElement('div');
            item.style.marginBottom = '4px';
            item.innerHTML = `<span style="color: #718096;">[${time}]</span> ${message}`;
            debugLogDiv.prepend(item);
            while (debugLogDiv.children.length > 100) {
                debugLogDiv.removeChild(debugLogDiv.lastChild);
            }
        }
        
        function speakText(text) {
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.rate = 0.9;
            utterance.pitch = 1.0;
            window.speechSynthesis.speak(utterance);
        }
        
        function float32ToWav(float32Array, sampleRate) {
            const buffer = new ArrayBuffer(44 + float32Array.length * 2);
            const view = new DataView(buffer);
            
            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + float32Array.length * 2, true);
            writeString(view, 8, 'WAVE');
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, 1, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * 2, true);
            view.setUint16(32, 2, true);
            view.setUint16(34, 16, true);
            writeString(view, 36, 'data');
            view.setUint32(40, float32Array.length * 2, true);
            
            let offset = 44;
            for (let i = 0; i < float32Array.length; i++, offset += 2) {
                const s = Math.max(-1, Math.min(1, float32Array[i]));
                view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
            }
            
            return new Blob([buffer], { type: 'audio/wav' });
        }
        
        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }
    </script>
</body>
</html>
